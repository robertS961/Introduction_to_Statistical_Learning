{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 9 Support Vector Machines\n",
        "\n",
        "* Support Vector Machine is a generalized version of maximal margin classifier. Though maximal margin classifier is simple it can't be used on most data sets, only ones that have a linear decision boundry.\n",
        "\n",
        "* We introduce the support vector classifier a more advanced verison of maximal margin classifier that can used in a wider range of cases.\n",
        "\n",
        "* Lastly support vector machine is a more complex version of support vector classifier can makes it able to support non linear cases.\n",
        "\n",
        "* Support vector machine is intended for classes which are binary(2 choices)"
      ],
      "metadata": {
        "id": "P3D8Gx5a8SIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.1 Maximal Margin Classifier"
      ],
      "metadata": {
        "id": "7bhIk0-g8Yvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1.1 Definining a Hyperplane\n",
        "\n",
        "* In P dimensional space a Hyperplane is a flat space of sub dimensions p -1.\n",
        "\n",
        "* Hyperplane in 2 dimensions would be $B_{0} + B_{1}X_{1} + B_{2}X_{2} = 0$\n",
        "\n",
        "* Hyper plane in p dimensions $B_{0} + B_{1}X_{1} + ...... +  B_{p}X_{p} = 0$\n",
        "\n",
        "* IF X statisfies the above equation $X = (X_{1},X_{2})$ so on for p dims then X lies in the hyperplane.\n",
        "\n",
        "* If X doesnt statisfie the equation it doesnt lie in the hyperplane and rather one of two sides. If $B_{0} + B_{1}X_{1} + ...... +  B_{p}X_{p} > 0$ then X lies on one side, if $B_{0} + B_{1}X_{1} + ...... +  B_{p}X_{p} < 0$ then X lies on the other. So we can simply calculate $B_{0} + B_{1}X_{1} + ...... +  B_{p}X_{p} = 0$ as a decision boundary for binary classification."
      ],
      "metadata": {
        "id": "hmGpuwG69pkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1.2 Classification Using a Seperating Hyperplane\n",
        "\n",
        "* n by p matrix X. We have two classes -1, and 1.\n",
        "\n",
        "* $f(x) = B_{0} + B_{1}X_{1} + ...... +  B_{p}X_{p} > 0, y_{i} =1$\n",
        "\n",
        "* $f(x) = B_{0} + B_{1}X_{1} + ...... +  B_{p}X_{p} < 0, y_{i} = -1$\n",
        "\n",
        "* So based on the sign of f(x) we can determine the clas. Also based on the magnitude of f(x) we can determine the confidence of the decision.\n",
        "\n",
        "* This of course leads to a linear decision boundary."
      ],
      "metadata": {
        "id": "gkxvGZYeBnyy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1.3 The maximal margin classifier\n",
        "\n",
        "* So which hyperplane do we pick to decide on the decision boundary?\n",
        "\n",
        "* There is an infinite number of choices. The best is the maximal margin hyperplane. The margin is the distance to the training points. So we want a line that maximizes the distantce to the training points.\n",
        "\n",
        "* When we then classify a test observation using the maximal margin hyperplane that is called the maximal margin classifier\n",
        "\n",
        "* Maximal margin hyperplane only relies on its support vectors for its decision point. The support vectors are the closest points of each class and are used to make the widest slab between them. If other points move beyond them the maximal margin hyperplane wouldn't move."
      ],
      "metadata": {
        "id": "OT3OyAswB8hz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1.4 Constructing of the Maximal Margin Classifier\n",
        "\n",
        "* The problem can be solved efficently with little computing power."
      ],
      "metadata": {
        "id": "HncQlL8-Ff2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1.5 The non -seperable Case\n",
        "\n",
        "* What if our data is seperable by a linear line. Like the data is mixed in with each other\n",
        "\n",
        "* There won't be an answer to the 9.1.4.\n",
        "\n",
        "* So we make a soft margin that almost seperates the classes. This generalization of the maximal margin classifier to non-seperable case is known as the support vector classifier"
      ],
      "metadata": {
        "id": "AHrCs8NPFlhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.2 Support Vector Classifier\n"
      ],
      "metadata": {
        "id": "V9iJjYyJHNCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2.1 Overview of the Support Vector Classifier\n",
        "\n",
        "* The maximal margin hyperplane can be over sensative to the support vectors and the addition of a point close to those can drastically change the decision boundary. THis can cause overfitting or an incorrect fit.\n",
        "\n",
        "* Also there can be an extremely small margin with the maximal margin hyerplane which is our confidence of the correct decision. This can make us unsure if the point is correctly predicted.\n",
        "\n",
        "* So we might pick a soft margin that fits the data slightly worse in exchange for more 1)more robustness of observation and 2)better classification of most points.\n",
        "\n",
        "* Now we let some observations be on the incorrect side of the margin of even the hyperplane."
      ],
      "metadata": {
        "id": "A-s0q1dZHYyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2.2 Details of Support Vector Classifier\n",
        "\n",
        "* The formula is similar to the maximal margin hyperplane except now we have added slack variables $ϵ_{i} $ .These slack variables must be $∑_{i=1}^{n} ϵ_{i} ≤C $ where we predetermine $C$.\n",
        "\n",
        "* If $ϵ_{i} = 0$ then it is on the correct side, If $ϵ_{i} > 0$ it has violated the margin, If $ϵ_{i} = > 1$ it has violated the margin and the hyerplane.\n",
        "\n",
        "* We usually determine C by cross validation as it is hand picked. A large C means the data is less fit so higher bias, but lower variance. Likewise is C is small than a lower bias but higher variance since the data is fit extremely tight.\n",
        "\n",
        "* Support vectors are the points that lie on the margin here or on the wrong side of the margin and hyperplane. These observations affect the support vector classifier. Thus C is only affected by the support vectors. Thus when C is large the margin will be large and there will be a lot of support vectors. Hence when C is small the margin will be smaller and there will be fewer support vectors.\n",
        "\n",
        "* This makes support vector classification equite robust because it only relies on the points in the margins. The points outside of these can change without any change to the support vector hyperplane.\n"
      ],
      "metadata": {
        "id": "kbcDtDnHHb4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.3 Support Vector Machines"
      ],
      "metadata": {
        "id": "kNVAKd_EI1br"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.3.1 Classification with Non - Linear Decision Boundaries\n",
        "\n",
        "* We can see that the above case of using support vector classification would be useless on a non linear decision boundard even with adjustedments to $C$. Therefore we should imploy our non linear terms used in Chapter 7 such as polynomials, splines, picewise, enlarging or shrinkening vector spaces.\n",
        "\n",
        "* We of course could enlarge the space with colinearity from chapter 2 and higher degree colinearity polynomials. However the function to computer the boundary line would become computationally expensive. This is where the support vector machine comes into play, as it aids us to exlarge the feature space in that way that leads to efficent computations."
      ],
      "metadata": {
        "id": "e1e8qEszLkOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.3.2 The Support Vector Machine\n",
        "\n",
        "* Enlarge the feature space in a specific way using kernals from the support vector classifier.\n",
        "\n",
        "* The kernal is transformation function applied to the dot product of the inner components of the maximal marginal classifier\n",
        "\n",
        "* There are two main kernals\n",
        "> 1. Polynomial Kernal - takes the prediction and actual dot product to a certain power. All observations matter equally\n",
        "> 1. Radial Kernal(Circular) - far observations carry less weight since it uses the euclidean distance from test to training. If it is larger than it won't effect the model much. It will effect it more if it is small so it realizes on more local points to the nonlinear boudary.\n",
        "\n",
        "* Advantages to the kernal over just enlarging the feature space is computation power. Kernals drastically lower the computation power needed as they use pairs and dot product compared to crazy large powers and colinearity.\n"
      ],
      "metadata": {
        "id": "HJ9BzDJrLwqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.3.3 An Application to Heart Disease Data\n",
        "\n",
        "* When running Support Vector Machines with polynomials make sure to try varying degrees of d. The power to which the kernal takes the dot product.\n",
        "\n",
        "* When running Support Vector Machines with radial kernels make sure to play around with different $γ$ values"
      ],
      "metadata": {
        "id": "AuCzJ0z7oYZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.4 SVMs with More Than 2 Clases\n",
        "\n",
        "* It is difficult to expand the hyperplane to more than 2 classes. However there is 2 main ways to fix this. *One versus One* or *One versus All*."
      ],
      "metadata": {
        "id": "98phpHTUohwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.4.1 One Versus One Classification\n",
        "\n",
        "* Assume we have $K> 2$ classes then Construct several Support Vector Machines $k \\choose 2$, each compares a pair of classes. Then for each test oberservation we tally which class it was assigned using the $k \\choose 2$ support vector machines. The class that tallied the most picks will take that test observation for that class."
      ],
      "metadata": {
        "id": "rH_Rwt8XqZsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.4.2 One Versus All Classification\n",
        "\n",
        "* Fit $K$ Support Vector Machines. Each time comparing the $K$ class to one of the remaing $K-1$ classes. Then for each test observation we plug it into each support vector machine. The one with the largest magnitude of $f(x)$ gets assigned to the corresponding class. Do this for all the test observations."
      ],
      "metadata": {
        "id": "v8W-yjywrytT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.5 Relationship to Logistic Regression\n",
        "\n",
        "* Both behave similar to data that is close to the predicted value. This means support vector machines only rely on the support vectors which make the margins. When determining the loss function we apply zero loss to values on the correct side of the hyperplane. Comparitively logistic regression applies very little loss to those points that are far from the decision boundary.\n",
        "\n",
        "* This is because they have a similar loss function. However support vector machines have an added penalty of $λ∑_{j=1}^{p} B_{j}^{2}$ much like the lasso.THe $λ$ represents C. If it is large than the margin is wider and the data is less fit.\n",
        "\n",
        "* It also uses kernal which logistic regression does not. However we can add powers and colinearity to logistic regression if so desired.\n",
        "\n",
        "* Lastly Support Vector Machines have a regression formula too. It is called support vector regression. It works like the least sum of squares except a loss penalty isn't applied if your sum of squares is below a certain threshold. This is an extension of the margin used in SVM."
      ],
      "metadata": {
        "id": "_g_4ieXDuSDK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V8GSR2l8uVhg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}