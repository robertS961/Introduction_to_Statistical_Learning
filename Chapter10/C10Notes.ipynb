{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 10 - Deep Learning\n",
        "\n",
        "* Deep learning is just neural networks but with more architectures and became more advanced since data sets grew.\n",
        "\n",
        "* Convolutional neural network for image classification\n",
        "\n",
        "* Recurrent neurral networks for time series and other sequences."
      ],
      "metadata": {
        "id": "B_f9h2KLUIZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.1 Single Layer Neural Networks\n",
        "\n",
        "* Features are the intput layer. The starting layer.\n",
        "\n",
        "* Hidden Layer is the next layer after the output layer. There is $K$ of them and this $K$ is picked by the user.\n",
        "\n",
        "* The outer layer is the result of the input and hidden layer. It is our final result function.\n",
        "\n",
        "* These K hidden nodes have the function $A_{1}....A_{k}$ and $A_{k} = h_{k}(X)$ where $h_{k} = g(z)$\n",
        "\n",
        "* The old choice for $g(z)$ is the logistic regression exponential function that turned linear regression into logistic regression. It is called the sigmoid function.\n",
        "\n",
        "* The current state of the art $g(z)$ is ReLU(*Rectified Linear Unit*)\n",
        "\n",
        "* Called a neural network since the sigmoid makes values relative to 0 or 1. So values close to 1 of the input node are firing while the values close to 0 are not. Much like neurons in the brain.\n",
        "\n",
        "* The model becomes non linear from the $g(x)$ function. It is linear through the input layer. However the $g(x)$ transforms these linear functions with sigmoid to non linearity.\n",
        "\n",
        "* The final formula will look along the lines of this $B_{0} + ∑_{k=1}^{K}B_{k}g(w_{0} + ∑_{j=1}^{p} w_{kj}X_{j})$ where $h(x) =g(w_{0} + ∑_{j=1}^{p} w_{kj}X_{j})$ Thus we must estimate the $B_{k}$ and the $w_{kj}$ we do this through least squares."
      ],
      "metadata": {
        "id": "VVog7rmMUOdd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.2 Multilayer Neural Networks\n",
        "\n",
        "* Modern day neural networks typically have more than one hidden layer and each hidden layer being of modest size.\n",
        "\n",
        "* With Multilayer neural networks the first input is the original input of the predictors. However after this the just discovered hidden layer takes on the input for the next hidden layer and so on til the final output layer.\n",
        "\n",
        "* Then $X_{j}$ also becomes $A_{k}^{h - 1}$ where $h$ represents the current hidden layer. This continues on for all $h$ and in the end $X$ has become quite transformed.\n",
        "\n",
        "* The final output step will go through the last hidden layer. Then take the softmax function basically the conditional probabilty of each digit being the answer. Then it takes the one with the highest probability.\n",
        "\n",
        "* So for classification we want to minimize *negative mulinomial loglike probability* and for regression we want to minimizes *least squares*.\n",
        "\n",
        "* These mulilayer neural networks do take a lot more processing power than logistic regression and linear disciminant analysis. However neural networks perform much better. The other issue to worry about is overfitting the data with too many hidden layers, to combat this we use ridge regression similar to the chapter 6 version or dropout regularization."
      ],
      "metadata": {
        "id": "LwQiK3KSXmGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.3 Convolutional Neural Networks\n",
        "\n",
        "* Neural networks bounced back in 2010 with image classification. Database were starting to capture tons of images. You can then use a $32 X 32$ pixels which have 3 values between 0 - 255 relating to red, blue, and green.\n",
        "\n",
        "* Convolutional Neural Networks (CNN) evolved to solve the class of problems like the one above.\n",
        "\n",
        "* It uses hidden layers to idenify small features. Then builds these small features through more hidden transformations into larger identifying features.\n",
        "\n",
        "* It does this through 2 main hidden layers. Convolution and Pooling. The convolution layers search for small patterns in the image. Then pooling downsamples to select a prominent subset. It then repeats this process a numerous number of times."
      ],
      "metadata": {
        "id": "jZQpcog6cuPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3.1 Convolution Layer\n",
        "\n",
        "* A Convolution layer is made up of a large number of Convolution filters. Each of which is a template to determine if a local feature is present in the image.\n",
        "\n",
        "* Convolution is a matrix equation of repeadly muliplying matrix elements and adding the results. Basically it takes the convolution filter and muliplies it all over the image. If the sub image is similar then it yields a large result and if it isnt then it yields a smaller result.\n",
        "\n",
        "* With Convolutional Neural Networks the filters are learning through the hidden layers."
      ],
      "metadata": {
        "id": "McMlHREKhaaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3.2 Pooling Layer\n",
        "\n",
        "* Max pooling operation condenses each non overlapping 2x2 pixel and takes the max value from there.\n",
        "\n",
        "* This condenses large images especially with the number we would have from the Convolution layer."
      ],
      "metadata": {
        "id": "9wV4NnQ7j05Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3.4 Architecture of a Convolutional Neural Network\n",
        "\n",
        "* First we take the original 3 channels(one for red, blue, and green) then perform a convolution on it. This produce 6 new images(6 since we had a 4x3 image and used a 2x2 filter. THen we use pooling to reduce the size of all 6 images. We continue this back and forth. Sometimes using muliple filters before pooling. EVentually we flatten the result and use our softmax activation to produce the probability of each pixel being used.\n",
        "\n",
        "* Inbetween these steps we might use ridge or dropout regression to reduce overfitting"
      ],
      "metadata": {
        "id": "BAPoO2iQj28m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3.4 Data Augmentation\n",
        "\n",
        "* Distorts the image at different steps to prevent overfitting. Also don't have to store a huge library of distorted images. This could be image transformations like shifting, shrinking, off axis the picture, left rotation , right rotation, etc.\n"
      ],
      "metadata": {
        "id": "Si7908WxlgVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3.5 Results using a Pretrained Classifier\n",
        "\n",
        "* Packages with already pretained images and require a ton less computing power. You can take these boiler plates and retrain them on the last few layers for specific image recognition."
      ],
      "metadata": {
        "id": "YDhmip9WmN-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.4 Document Classification"
      ],
      "metadata": {
        "id": "yOENEEhBm-zx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4QR26j7JnBoY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}