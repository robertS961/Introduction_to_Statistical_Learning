{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 10 - Deep Learning\n",
        "\n",
        "* Deep learning is just neural networks but with more architectures and became more advanced since data sets grew.\n",
        "\n",
        "* Convolutional neural network for image classification\n",
        "\n",
        "* Recurrent neurral networks for time series and other sequences."
      ],
      "metadata": {
        "id": "B_f9h2KLUIZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.1 Single Layer Neural Networks\n",
        "\n",
        "* Features are the intput layer. The starting layer.\n",
        "\n",
        "* Hidden Layer is the next layer after the output layer. There is $K$ of them and this $K$ is picked by the user.\n",
        "\n",
        "* The outer layer is the result of the input and hidden layer. It is our final result function.\n",
        "\n",
        "* These K hidden nodes have the function $A_{1}....A_{k}$ and $A_{k} = h_{k}(X)$ where $h_{k} = g(z)$\n",
        "\n",
        "* The old choice for $g(z)$ is the logistic regression exponential function that turned linear regression into logistic regression. It is called the sigmoid function.\n",
        "\n",
        "* The current state of the art $g(z)$ is ReLU(*Rectified Linear Unit*)\n",
        "\n",
        "* Called a neural network since the sigmoid makes values relative to 0 or 1. So values close to 1 of the input node are firing while the values close to 0 are not. Much like neurons in the brain.\n",
        "\n",
        "* The model becomes non linear from the $g(x)$ function. It is linear through the input layer. However the $g(x)$ transforms these linear functions with sigmoid to non linearity.\n",
        "\n",
        "* The final formula will look along the lines of this $B_{0} + ∑_{k=1}^{K}B_{k}g(w_{0} + ∑_{j=1}^{p} w_{kj}X_{j})$ where $h(x) =g(w_{0} + ∑_{j=1}^{p} w_{kj}X_{j})$ Thus we must estimate the $B_{k}$ and the $w_{kj}$ we do this through least squares."
      ],
      "metadata": {
        "id": "VVog7rmMUOdd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.2 Multilayer Neural Networks\n",
        "\n",
        "* Modern day neural networks typically have more than one hidden layer and each hidden layer being of modest size.\n",
        "\n",
        "* With Multilayer neural networks the first input is the original input of the predictors. However after this the just discovered hidden layer takes on the input for the next hidden layer and so on til the final output layer.\n",
        "\n",
        "* Then $X_{j}$ also becomes $A_{k}^{h - 1}$ where $h$ represents the current hidden layer. This continues on for all $h$ and in the end $X$ has become quite transformed.\n",
        "\n",
        "* The final output step will go through the last hidden layer. Then take the softmax function basically the conditional probabilty of each digit being the answer. Then it takes the one with the highest probability.\n",
        "\n",
        "* So for classification we want to minimize *negative mulinomial loglike probability* and for regression we want to minimizes *least squares*.\n",
        "\n",
        "* These mulilayer neural networks do take a lot more processing power than logistic regression and linear disciminant analysis. However neural networks perform much better. The other issue to worry about is overfitting the data with too many hidden layers, to combat this we use ridge regression similar to the chapter 6 version or dropout regularization."
      ],
      "metadata": {
        "id": "LwQiK3KSXmGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.3 Convolutional Neural Networks\n",
        "\n",
        "* Neural networks bounced back in 2010 with image classification. Database were starting to capture tons of images. You can then use a $32 X 32$ pixels which have 3 values between 0 - 255 relating to red, blue, and green.\n",
        "\n",
        "* Convolutional Neural Networks (CNN) evolved to solve the class of problems like the one above.\n",
        "\n",
        "* It uses hidden layers to idenify small features. Then builds these small features through more hidden transformations into larger identifying features.\n",
        "\n",
        "* It does this through 2 main hidden layers. Convolution and Pooling. The convolution layers search for small patterns in the image. Then pooling downsamples to select a prominent subset. It then repeats this process a numerous number of times."
      ],
      "metadata": {
        "id": "jZQpcog6cuPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3.1 Convolution Layer\n",
        "\n",
        "* A Convolution layer is made up of a large number of Convolution filters. Each of which is a template to determine if a local feature is present in the image.\n",
        "\n",
        "* Convolution is a matrix equation of repeadly muliplying matrix elements and adding the results. Basically it takes the convolution filter and muliplies it all over the image. If the sub image is similar then it yields a large result and if it isnt then it yields a smaller result.\n",
        "\n",
        "* With Convolutional Neural Networks the filters are learning through the hidden layers."
      ],
      "metadata": {
        "id": "McMlHREKhaaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3.2 Pooling Layer\n",
        "\n",
        "* Max pooling operation condenses each non overlapping 2x2 pixel and takes the max value from there.\n",
        "\n",
        "* This condenses large images especially with the number we would have from the Convolution layer."
      ],
      "metadata": {
        "id": "9wV4NnQ7j05Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3.4 Architecture of a Convolutional Neural Network\n",
        "\n",
        "* First we take the original 3 channels(one for red, blue, and green) then perform a convolution on it. This produce 6 new images(6 since we had a 4x3 image and used a 2x2 filter. THen we use pooling to reduce the size of all 6 images. We continue this back and forth. Sometimes using muliple filters before pooling. EVentually we flatten the result and use our softmax activation to produce the probability of each pixel being used.\n",
        "\n",
        "* Inbetween these steps we might use ridge or dropout regression to reduce overfitting"
      ],
      "metadata": {
        "id": "BAPoO2iQj28m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3.4 Data Augmentation\n",
        "\n",
        "* Distorts the image at different steps to prevent overfitting. Also don't have to store a huge library of distorted images. This could be image transformations like shifting, shrinking, off axis the picture, left rotation , right rotation, etc.\n"
      ],
      "metadata": {
        "id": "Si7908WxlgVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3.5 Results using a Pretrained Classifier\n",
        "\n",
        "* Packages with already pretained images and require a ton less computing power. You can take these boiler plates and retrain them on the last few layers for specific image recognition."
      ],
      "metadata": {
        "id": "YDhmip9WmN-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.4 Document Classification\n",
        "\n",
        "* This will involve predicting attirbutes of documents. Such as based on a review was the movie good or bad\n",
        "\n",
        "* Common strategy is to use a *bag of word* certain words have a negative or positive meaning. We have a vector $M$ and we mark 1 at each word that is used.\n",
        "\n",
        "* With these vectors, lets call it matrix $M$ it contains mostly zeros with a few 1s pertaining to the positive or neg words. Thus it is called a sparse matrix format since we can store it more efficently by just storing the location of the ones.\n",
        "\n",
        "* Bag of words model struggles tho with context as \"blissfully short\" could be a negative review even tho blissfully is a positive word. There are 2 methods to deal with context\n",
        "\n",
        "> 1. *bag-of-ngrams* This method takes all distinct phrases of size n and then categorizes them as positive or negative\n",
        "> 1. *sequence* treat the document as a sequence and taking into context the words that follow and preceed. More to come about the sequence exmaple."
      ],
      "metadata": {
        "id": "yOENEEhBm-zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.5 Recurrent Neural Networks\n",
        "\n",
        "* Many data sources are sequential in nature and call for special treatment when building predictive models. Like writing such as articles or movies reviews, time series of weather, financial time series, recorded speech, handwriting into digital.\n",
        "\n",
        "* In a recurrent neural network (RNN) the object is a sequence. The output is usually a scalar but can sometimes be another sequence.\n",
        "\n",
        "* The neural network differs from the CNN(convolutional neural network) even tho both have an input layer, hidden layer, and output layer. But the ordering if different. In the recurrent neural network the input is fed to a hidden layer. This layer then produces an output. However the knowledge gained is spread to the second hidden input layer, which also takes in a new input. It uses the new input, its hidden function, and the previous knowledge from the last hidden function output to produce another output. We do this for a certain number of iterations then take the last generated output."
      ],
      "metadata": {
        "id": "4QR26j7JnBoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.5.1 Sequential Models for Document Classification\n",
        "\n",
        "* Normally we use hot encoding for each word in the spacce. Hence if there is 10,000 words than a 1 followed by 9999 zeros. However a more popular and efficent approach is the embedding space. This represents each word in a much lower dimensional space. So we will instead represent each word with m real numbers, none of which are typically zero. M is usually around the size low 100's.\n",
        "\n",
        "* We have can have our neural network learn $E$ which is m(10000) since we have 10000 words. This way it understands the encodement. This is called *embedding layer*.\n",
        "\n",
        "* Or we can precomute $E$ and insert it in. This is called *weight freezing*.\n",
        "\n",
        "* The next step would be to pick a size $L$. Make sure each document is at least length $L$ so we have enough knowledge to train and test on. If it is shorter than pad it, longer then trim the start.\n",
        "\n",
        "* We know then the $n$ observations of each size $L$ into the RNN. Follow the steps in 10.5\n",
        "\n",
        "* We can improve the model by adding in short and long term memory. Instead of just taking the knowledge from the previous layer, it can take many previous layers results. It then balances all of them to make sure the most recent layer isn't washed out."
      ],
      "metadata": {
        "id": "-DWfRjNTNstQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.5.2 Time Series Forcasting\n",
        "\n",
        "* Time series is a series that relies on time.\n",
        "\n",
        "* Time series can sometimes have correlation with time compared to independent features. Check this with the data to see if there is a strong correlation with time if there is then the data is not independent"
      ],
      "metadata": {
        "id": "OisBRy0DS6gw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RNN Forcasting\n",
        "\n",
        "* This time we don't have words for our predictors. Instead we pick a length $L$ like before and take $n$ observations(These observations can overlab by $L-1$). The $n$ observations are short time series of length $L$. Then we pick the response at the end of the length $L$ to predict. Feed this into a recurrent neural network.\n"
      ],
      "metadata": {
        "id": "oGmVaE9OUo30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Autoregression\n",
        "\n",
        "* Pretty much linear matrix muliplication to discover all the coefficients. Thus a solution vector $y$ is fit onto a matrix $M$. Where $M$ represents vectors of 4 days and y represents the 5th day."
      ],
      "metadata": {
        "id": "6_xCncicVv2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.5.3 Summary of RNN's\n",
        "\n",
        "* Other methods not covered here about RNN. One approach uses the embedded coding but represents the vector has an image then run it through convolutional neural networks.\n",
        "\n",
        "* Also *seq2seq* which is great for language translation. Instead of going forward to backwards like we did it goes both ways to capture more meaning.\n",
        "\n",
        "* Luckily Pytorch, google translate have developed beautiful tools for us to use in our everyday life, trained off massive databases."
      ],
      "metadata": {
        "id": "C7tRlvkuXQeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.6 When to Use Deep Learning\n",
        "\n",
        "* The simplier linear models can sometimes produce better or even the same results with way less computational power, less parameter changing, and easier explination. So go with Occam's Razor when you can\n",
        "\n",
        "* Always try to fit the simpliest model first as it will likely be easier to fit, explain, and potientally less fragile. And think back to our complexity/performance trade off.\n",
        "\n",
        "* Most likely want deep learning when our training set is extremely large and interpretability of the mode is not a priority."
      ],
      "metadata": {
        "id": "ylS2z_PHYT7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.7 Fitting a Neural Network\n",
        "\n",
        "* To protect neural networks from being overfit we employ two different methods\n",
        "> 1. *Slow Learning* - Model is fit in a somewhat slow process using gradient decsent. Fitting is stopped when overfitting is detected\n",
        "> 1. *Regularization* - Penalities are impossed on the parameters like ridge regression and the lasso.\n",
        "\n",
        "* Gradient decsent is like starting on top of a mountain and looking for the lowest point around you and taking steps to get there. You want to pick gradients off the curve that are decreasing(thus you are moving towards a local minimum). Once you reach the local minimum the derivative will be 0 and start to decrease . You will stop there since you can't make the gradient decrease anymore.\n"
      ],
      "metadata": {
        "id": "SY6acevsYYfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.7.1 Backpropogation.\n",
        "\n",
        "* When we take the partial derivatives in terms of what we want to minimizes which is the $β_{k}$ and $w_{kj}$. We will use the produce rule which is called backpropogation."
      ],
      "metadata": {
        "id": "V7D2V8CAaVd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.7.2 Regularization and Stochastic Gradient Descent\n",
        "\n",
        "* Stochastic Gradient Descent is where we do normal gradient descent except we use a small batch of the $n$ observations instead of $n$.\n",
        "\n",
        "* Data sets that are larger need regularization to not overfit so we bring back the $λ$ from lasso and ridge. This will penalize terms to prevent overfitting of parameters.\n",
        "\n",
        "* *epocho* is commonly used and means how many times the full training set has been used. AS epocho increases the chance for overfitting increases too. Normally a threshold of epocho to stop the training at."
      ],
      "metadata": {
        "id": "RwkxQY9NfE_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.7.3 Dropout Learning\n",
        "\n",
        "* Much like random forest, a random selection of nodes are dropped out to prevent overfitting. The nodes missing are made up for by the other nodes and their weights are increased."
      ],
      "metadata": {
        "id": "Q-6Rpb7xgfNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.7.4 Network Tunning\n",
        "\n",
        "1. *The number of hidden layers and number of units per hidden layer* - the modern meta is a large number of units per layer and use modern day overfitting prevention\n",
        "1. *Regularization of tunning parameters* - these include the $λ$ value per parameter for ridge and lasso and the dropout rate from dropout learning. They are typically set sperately at each layer\n",
        "1. *Details of stochastic gradient descent* - These include batch size, number of epochos, and details of augmentation.\n",
        "\n"
      ],
      "metadata": {
        "id": "MrZwjpukg2n0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.8 Interpolation and Double Descent\n",
        "\n",
        "* Basically with some neural networks overfitting them to some extent results in a inverted $U$. This means there is indeed a double descent and we can get low test error rates from overfitting. This doesn't occur in every case.\n",
        "\n",
        "* Normally you need more fitting then there are observations but it can happen with less. The best way to check is to test it on the validation set and see.\n",
        "\n",
        "* This practice can be avoided by proper punishment of paramets like ridge or lasso. Also correctly assigned epocho termination size etc.\n",
        "\n",
        "* Bias - Variance still holds true and it is still better to stop early."
      ],
      "metadata": {
        "id": "H5jwzYDTh-Ns"
      }
    }
  ]
}