{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 8 Tree Based Methods\n",
        "\n",
        "* Know as decision trees since the rules used to segmented predictor space can be summarized in a tree\n",
        "\n",
        "* Tree method splits or stratisfies the predictor space into several smaller regions. Then it normally uses the mean or mode of those smaller spaces for predictions\n",
        "\n",
        "* Tree based methods aren't as competitive as the methods learned in chapter 6 and 7. So we will introduce bagging, random forest, boosting, bayesian additive regression trees. These methods take the normal approach of one tree but instead use several then combined them for one consesus.\n",
        "\n",
        "* Several trees can usually boost performance accuracy at the cost of interpertation."
      ],
      "metadata": {
        "id": "pdSygKKssIRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1 The Basics of a Decision Tree\n",
        "\n",
        "* Trees can be used for regression and classification problems"
      ],
      "metadata": {
        "id": "_kIK3IBnsPA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1.1 Regression Trees\n",
        "\n",
        "* Decision Trees are typically drawn upside down.\n",
        "\n",
        "* The groups are called nodes or terminal nodes.\n",
        "\n",
        "* Futher splits of the predictor space inside the tree are called internal nodes\n",
        "\n",
        "* The area in the tree that connects the nodes are called branches\n",
        "\n",
        "* These trees might be predict as well but they are easy to interpt and contain a nice graphical representation."
      ],
      "metadata": {
        "id": "icfQS_BFuHzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prediction via Stratification of the Feature Space\n",
        "\n",
        "* To build a regression tree there is two steps\n",
        "\n",
        ">1. We divide the predictor space $X_{1}.... X_{p}$ into $J$ distinct nonoverlapping regions $R_{1}......R_{J}$\n",
        ">1. Every observation that falls into $R_{j}$ we make the same prediction which is the mean of the response values for the training observations in $R_{j}$\n",
        "\n",
        "* Our goal to make splits to minimize the RSS. Of course it is computationally infeasible to brute force this for large data sets. Instead we use recursive binary splitting.\n",
        "\n",
        "* Recursive binary splitting is a top down approach since we start at the top of the tree(point where all the predictors belong to the same region.) The approach is greedy since it picks the best split at each step rather than looking at the best split further down the tree.\n",
        "\n",
        "* To perform recursive binary splitting we do the following:\n",
        "> 1. We pick a predictor $ X_{j}$  and a cut point $ s $. Such that splitting the predictor space yeilds {$ X| X_{j} < s $} and {$X| X_{j} ≥ s$} the greatest possible reduction in RSS. So consider all possible predictors then for each predictor consider all possible cut points.\n",
        "> 1. Pick $(j,s)$ values to minimize the following equation. $∑_{R_{1}(j,s)}(y - ŷ_{R_{1}})^{2} + ∑_{R_{2}(j,s)}(y - ŷ_{R_{2}})^{2}$\n",
        "> 1. We repeat this process but instead of looking at the entire space we look at the previous definined regions.\n",
        "> 1. We continue this til a criteria is met like no more than 5 observations per region.\n",
        "> 1. Once the tree is build we use it to predict the response for a given test observation using the mean of training observation from the region which the test observation is from.\n"
      ],
      "metadata": {
        "id": "WL3O5TDbvy6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tree Pruning\n",
        "\n",
        "* The above tree will likely perform well on the training data but poorly on the test data. It will most likely overfit and have too large of a variance.\n",
        "\n",
        "* To fix this we can make sure we only split if the decrease in RSS is greater than a large threshold. This will lead to smaller trees but is still greedy. As a huge decrease in RSS in a few branches could be reached but we might terminate early.\n",
        "\n",
        "* A better strategy is to build a huge tree then prune it by removing features to get a sub tree.\n",
        "\n",
        "* However how do we know how much to prune or what size subtree to select? We could use cross validation or a validation set. But it would be computationally expensive.\n",
        "\n",
        "* We use Cost Complexity Pruning aka weakest link pruning. Rather than considering all possible subtrees, we instead consider trees indexed by a nonnegative tunning parameter α. For each value α there corresponds a subtree $T ⊂ T_{0}$ such that $ ∑_{m =1} ^{|T|}∑_{R_{m}}(y- ŷ_{R_{m}})^{2} + α|T|$ is minimized. $|T|$ represents that count of terminal nodes so as α increases there will be a penalty to pay much like the Lasso.\n",
        "\n",
        "* So we pick a value alpha and calculate the most optimal subtree that minimizes RSS. We can use a validation set or cross validation to calculate the best value of alpha."
      ],
      "metadata": {
        "id": "UCELFwX9xIzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1.2 Classification Tree\n",
        "\n",
        "* Much like regression trees except we are trying to predict which class it belongs too.\n",
        "\n",
        "* Based on the region it belongs to the most commonly occuring class there is the prediction.\n",
        "\n",
        "* We build the model like we did for a regression tree except we can't use RSS. Instead we use classification error rate.  This is the fraction of classes that don't match the most common observation. $E = 1 − max(p̂_{mk})$\n",
        "\n",
        "* Turns out this method isn't as senative as other measures like  *Gini Index* which is defined as $G = ∑_{k=1}^{K}p̂_{mk}(1 − p̂_{mk})$, where $p̂_{mk}$ represents a proportion of data in the $m$ region belonging to the $k$th class. So if this proportion is close to 0 or 1 for $K$ classes then G takes on a small number. The above formula represents node purity.\n",
        "\n",
        "* Another alternative is *entropy* given by $D = -∑_{k=1}^{K}p̂_{mk}logp̂_{mk})$. So it can never exceed 0 based on log properties. Once again it will take on a small value if the proportions are all close to 0 or all close to 1. It is very similar to *Gini* since it is a node purity algorithm.\n",
        "\n",
        "* When creating the tree it is prefered to use Gini or entropy then when pruning the tree you should use classification error rate.\n",
        "\n",
        "* You can also split branches based on qualitive variables too. Just assigned a certain subset to the left branch and the rest to the right branch."
      ],
      "metadata": {
        "id": "TqaV_Pve5xWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1.3 Tree vs Linear Models\n",
        "\n",
        "* Which one to pick and when? Well if the data looks linear than you should probably use a linear model. However if the data is non linear and has complex relationships than use regression tree.\n",
        "\n",
        "* Regression trees do make the data more interpretable if that is a goal.\n",
        "\n",
        "* Regardless assess boths performance using cross validation and validation sets and see which performs better."
      ],
      "metadata": {
        "id": "Y1TALSrb7lTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1.4 Advantage and Disadvantages of Trees"
      ],
      "metadata": {
        "id": "8p873EhGAnSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pros:\n",
        "> 1. They are super easy to explain to people and to visualize compared to linear regression and graphing linear regression.\n",
        "> 1. Trees can handle qualative variables instead of having to hot encode or create dummy variables\n",
        "Cons:\n",
        "> 1. Usually don't have the same level of prediction accuracy as some of the classification and regression techniques.\n",
        "> 1. Trees can be non robust so a small change in data can cause a large change in the estimation for the tree. However by using bagging, random forest, and boosting the tree models can be substantially improved by using muliple trees."
      ],
      "metadata": {
        "id": "c4mqT1laBaex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.2 Bagging , Random Forest, Boosting, and Bayesian Additive Regression Trees\n",
        "\n",
        "* Ensemble method is an approach that combines many  simplier building block models in order to obtain a more powerful one. Building block models can be called weak leaners since on their own they don't give powerful results. The bagging, random forest, and boosting are ensemble methods and the building block methods are a regression or classification tree."
      ],
      "metadata": {
        "id": "S30nFgDOCXfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2.1 Bagging\n",
        "\n",
        "* Alot of the trees suffer from high variance and a way to lower that is bootsrapping or bagging.\n",
        "\n",
        "* In an ideal world we would take $n$ training sets to get $f̂ _{avg}(x) = \\frac {1}{B}∑_{b = 1}^{B}f^{̂ b}(x)$\n",
        "\n",
        "* However we don't have $n$ training sets so instead we use bootstraping to create training sets. Hence it becomes: $f̂_{avg}(x) = \\frac {1}{B}∑_{b = 1}^{B}f^{̂ *b}(x)$\n",
        "\n",
        "* Thus we create B trees and do not prune them. Then we perform bagging on all B trees. Each B tree has high variance but low bias. Combine all the trees with bagging and we lower the variance and keep the bias relatively low.\n",
        "\n",
        "* Bagging works on qualative response too. You simply create the B trees and use the most commonly occuring class as the $f$ function.\n",
        "\n",
        "* We need to pick a large B as it will not lead to overfitting . A large enough B will allow us to lower the variance."
      ],
      "metadata": {
        "id": "4pFBR4ReCgvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Out of the bag error estimation\n",
        "\n",
        "* It can be shown that we use only around 2/3 of the tree per bagged $T$. Thus we don't need to use CV or Validation set. As we can use the last 1/3 as the test set to calculate the MSE or classification error. This will reduce a ton of pc power."
      ],
      "metadata": {
        "id": "5PV1RmumRP-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Variable Importance Measures\n",
        "\n",
        "* However once we start using several trees our readibility and intereptability go down. We can use graphs or summaries to show the RSS drop for regression of each variable and the gini index decrease for each variable for classification. Which makes it easy to see the important variables."
      ],
      "metadata": {
        "id": "MPIwJFP8TS5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2.2 Random Forest\n",
        "\n",
        "* Has an advantage on bagging and usually performs slightly better.\n",
        "\n",
        "* Still create B trees from the training set using bootstraping and baggings. However when building these tree you don't use the full set of $p$ predictors. Instead you use a subset $m$ of these predictors. M is usually the size of $\\sqrt{p}$\n",
        "\n",
        "* This has an advantage. Think of a strong predictor that will always be choosen first there those B trees will be strongly correlated since they all will pick that predictor first. However if you remove that predictor then a different one can be chosen first. This makes the trees decorrelated, lowering the variance and making the models more reliable.\n",
        "\n",
        "* You can't overfit with this example so you should use a sufficently high B. Also you should try different values of $m$."
      ],
      "metadata": {
        "id": "OwTAyYPdTziy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2.3 Boosting\n",
        "\n",
        "* Another method to improve upon Random Forest. Like bagging and random forest we grow B trees(careful we can overfit if B is too large unlike the others). However the trees are grown sequentially. Meaning each tree is grown on the information of the others\n",
        "\n",
        "* Also fit the trees not on the response y but on the residuals. And are all set to the same depth $d$ specified by the algorithm usually small and sometimes even 1, a stump.\n",
        "\n",
        "* There is also a λ parameter so that the model learns slowly.\n",
        "\n",
        "* Algo.\n",
        ">1. $1....B$ Trees with d+1 terminal splits.\n",
        ">>1. At each step update $f^{̂}$ by adding the new shrunked version to it. $f^{̂}(x) = f^{̂}(x) + λf^{̂b}(x)$\n",
        ">>1. Update the residuals for all i in the training set. $r_{i} = r_{i} + λf^{̂b}(x_{i})$\n",
        "1. After the loop completes them sum the tree functions. $f^{̂}(x) = ∑_{b=1}^{B}λ f^{̂b}(x)$"
      ],
      "metadata": {
        "id": "U4QazMbxT6FB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2.4 Bayesian Additive Regression Trees\n",
        "\n",
        "* Like a combination of random forest and boosting. We take from random forest that we will use bootstraping and randomness and we take the weighted trees from boosting. We will make a new tree then for $1...K$ we will pick a random perturbations that maximizes the imrovement for residual of the previous tree.\n",
        "  * The pertubation has 2 rules\n",
        "  1. We can prune it or add branches\n",
        "  1. We can change prediction in each terminal node\n",
        "* Full Algo is as follows:\n",
        ">1. Pick a K value and set all the $f_{1}^{̂1}(x) = .... = f_{K}^{̂1} = \\frac {1}{nK}∑_{i=1}^{n}y_{i}$\n",
        ">1. Compute $f^{̂1}(x) = ∑_{k = 1} ^{K}f_{k}^{̂1}= \\frac{1}{n}∑_{i=1}^{n}y_{i}$\n",
        ">1. For $b = 2....B$\n",
        ">>1. For $ k= 1... K$\n",
        ">>>1. For $ i= 1....n$ compute the current partial residual $r_{i} = y_{i} - ∑_{k < k^{`}} f_{k^{`}}^{̂b}(x_{i}) - ∑_{k > k^{`}} f_{k^{`}}^{̂b -1}(x_{i})$\n",
        ">>>1. Fit a new tree $f_{k^{`}}^{̂b}(x_{i})$ to $r_{i}$ by randomly pertubating the $kth$ tree from the previous iteration $ f_{k^{`}}^{̂b -1}(x_{i})$ Want to select the best pertubation that improves the fit the most.\n",
        ">>1. Computer $f^{̂b}(x) = ∑_{k=1}^{K} f_{k}^{̂b}(x)$\n",
        ">1. Compute the mean after L burn in samples $f^{̂ }(x) = \\frac {1}{B -L} ∑_{B = L + 1} ^{B} f^{̂B}(x)$\n",
        "\n",
        "* When fitting park we must select the number of trees $K$, number of iterations $B$ and the number of burn ins $L$. Normally chose large values for $B$ and $K$ and an moderate value for $L$.\n",
        "\n",
        "* This method normally performs well out of the box - that is, it performs well without much tuning."
      ],
      "metadata": {
        "id": "Xk2p_AtGoa4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2.5 Summary of the Tree Ensemble Methods\n",
        "\n",
        "* Bagging - Grown on random samples of the data. Uses bootsrapping to create several samples then group them together. This causes the trees to be correlated and are relatively similar so can get trapped by local optima and fail to thorughly explore the space.\n",
        "\n",
        "* Random Forest - Grown on random samples of the data. Trees are once again grown indepently however use a random subset of predictors therefore decorrelating the trees and exploring the space better.\n",
        "\n",
        "* Boosting - grown on the orginial data. The trees are grown successively from the previous trees. The new tree is grown from a signal of the previous trees and shrunked down before it is used.\n",
        "\n",
        "* BART - Once again grow the trees from the orginial data and do it successively. However each tree is perturbed in order to avoid local minima and better explore the space."
      ],
      "metadata": {
        "id": "naRVvu-0uK4V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TR_5wbWm1XBy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}