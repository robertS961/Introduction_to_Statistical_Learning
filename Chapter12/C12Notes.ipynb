{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 12 Unsupervised Learning\n",
        "\n",
        "* Book has primarily involved supervised learning like regression and classification. Where we have a response $Y$ for a set of $n$ observations and $p$ predictors.\n",
        "\n",
        "* Now we will not have a response $Y$ and instead look to find meaningful observations about the data.\n",
        "\n",
        "* Main 2 types we will focus on are\n",
        "  * principal component analysis - tool used for data visualization\n",
        "  * clustering - method for discovering subgroups in the data"
      ],
      "metadata": {
        "id": "Apmm2_c61B0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.1 The Challenge of Unsupervised Learning\n",
        "\n",
        "* We can't check our work like we did with supervised learning. We don't have an outcome variable.\n",
        "\n",
        "* Unsupervised learning can be useful in such cases\n",
        "\n",
        "  * We know a shoppers previous online shopping history and want to show similar items that would group in with those\n",
        "  * We examine the genes of breast cancer patients and try to find a grouping of potentially bad genes\n"
      ],
      "metadata": {
        "id": "lJR7fhOT1KXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.2 Principal Component Analysis\n",
        "\n",
        "* Similar to chapter 6 Principal Component Regression. When we do this regression we must select the principal components.\n",
        "\n",
        "* PCA - (Principal Component Analysis) -  is the process of selecting these principal components\n",
        "\n",
        "* Remember PCR goal is to summarize the space with the fewest variables for the largest space."
      ],
      "metadata": {
        "id": "DgliB-T3HeZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.2.1 What are principal components\n",
        "\n",
        "* One way to visualize correlation is to use scatter plots. However, if $p$ is large then $p\\choose 2$ will be unbearable. If $p$ is small then this is doable.\n",
        "\n",
        "* We would want to capture as much as the data as possible in a smaller dimension.\n",
        "\n",
        "* PCA does this as it tries to capture a smaller amount of the interesting predictors. It finds predictors more interesting the more they vary across each dimension.\n",
        "  * Each of the dimensions found by PCA is a linear combination of the all $p$ features\n",
        "  * The first principal component of a set of features $X_1 ,....., X_p$ is the normalized combination of the features $Z_1 = ϕ_{11}X_1 + ϕ_{21}X_2+....+ ϕ_{p1}X_p$ and normalized means $ ∑_{j=1}^{p} Φ_{j1}^2 = 1$. This limits the size of the values so the variance isn't extremely large.\n",
        "  * We refer to $Φ_{11}$ as the loadings and $ϕ_{1} = (ϕ_{11}+ ϕ_{21}+....+ ϕ_{p1})^T$\n",
        "  * Imagine an  n x p  matrix. Look for linear combinations of sample feature of the form  $z_{i1} = ϕ_{11}x_{11} + ϕ_{21}x_{21}+....+ ϕ_{p1}x_{p1}$ subject to the constraint listed above.\n",
        "  * Thus we must maximize the following equation for $ϕ_{11}+ ϕ_{21}+....+ ϕ_{p1}$ ,Max = $\\frac {1}{n} ∑_{i=1}^n(∑_{j=1}^p ϕ_{j1}x_{ij})^2$ subject to the constraint above\n",
        "  * The Second Principal Component is the linear combination of $X_1 ...... X_p$ that has the max variance out of all the linear combinations that are uncorrelated to $Z_1$ gives us $Z_2$\n",
        "  * $z_{i2} = ϕ_{12}x_{i1} + ϕ_{22}x_{i2}+....+ ϕ_{p2}x_{ip}$\n",
        "  * To be uncorrelated $Z_1$ must be orthogonal to $Z_2$.\n",
        "  * Once we have computed all principal components we can plot them against each other. This greatly reduces the amount of variables to plot.\n",
        "  * Can also determine the values for $PC1$ and $PC2$. If the numbers are large and similar then that predictor is important and correlated. If it is less than 0 then it is below average, and 0 is standard."
      ],
      "metadata": {
        "id": "_JtAF6IeJJpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.2.2 Another Interpretation of Principal Components\n",
        "* Our goal is to make a hyperplane with n principal components in n dimensions. We will minimize Euclidean distance. The smallest sum of squares will be our answer\n",
        "* This would give us a formula to find the smallest value of the following equation where $M = $ number of principal components.\n",
        "  * $ ∑_{i=1}^n∑_{j=1}^p (x_{ij} - ∑_{m=1}^M z_{im}ϕ_{jm})^2 $\n",
        "* This is a great method when M is larger and be perform better than the variance formula above"
      ],
      "metadata": {
        "id": "rUk2J9vSJqZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.2.3 The proportion of variance explained\n",
        "* We want to understand how much information is lost in projecting the observations onto the first few principal components. Aka how much variance is not contained in the first few principal components.\n",
        "  * More generally we are interested in the proportion of variance explained (POE)\n",
        "* The PVE can be interpreted as the $R^2$ value .\n",
        "* Per each Principal Component it will explain less variance of the data but cumulating all $M$ principal components should eventually explain 100 % of the variance.\n"
      ],
      "metadata": {
        "id": "wULIg7pidanI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.2.4 More on PCA"
      ],
      "metadata": {
        "id": "sjUHoxighG1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scaling the Variables\n",
        "* The variables need to have a mean of 0\n",
        "* Variables should also be multiplied by a constant since their units can all be different. This would lead to several different variances and we want the variances to be relatively the same. This way the results per principal component are judged fairly.\n",
        "* If all variables were measured in the same units then we might not want to scale everything to standard deviation 1.\n",
        "* We scale everything to standard deviation 1 as it is difficult to arbitrarily scale units of different magnitudes."
      ],
      "metadata": {
        "id": "u69PtdZJ11r1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Uniqueness of Principal Components\n",
        "* Principal Components are almost always unique and the sign bit doesn't matter. IT can be negative or positive. The result is the same."
      ],
      "metadata": {
        "id": "D7FmCmRM3Glk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deciding how many Principal Components to use\n",
        "* One way to is to plot the amount of variance each Principal component explains and stop pick a point where it drops off.\n",
        "\n",
        "* Another is to explore the data til we stop seeing uniqueness from the principal components and stop it there."
      ],
      "metadata": {
        "id": "ajw1an563sha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.2.5 Other Uses for Principal Components\n",
        "* Many other techniques like regression , classification, and clustering you can use n x M instead of n x p. Where M is the principal component score vector. This reduces the noise in the data and makes computations less intensive."
      ],
      "metadata": {
        "id": "0ltaPq8h5Fk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.3 Missing Values and Matrix Completion\n",
        "* First we could remove the entire row if an element $x_{ij}$ is missing\n",
        "\n",
        "* This is awful wasteful so instead we could replace that point by the mean of the $j$ column. This works well but there is better ways.\n",
        "\n",
        "* We can use principal components in a method called matrix completion to fill in the missing data.\n",
        "\n",
        "* This works well with recommender systems to recommend users new movies or products."
      ],
      "metadata": {
        "id": "2yweI4U05yFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principal Components with Missing Values\n",
        "* We go back to the previous section of computing Principal components and use that equation. However, it becomes much more difficult to solve since we can't use eigenvalues and vectors since there is missing values.\n",
        "\n",
        "* Once we solve that equation we can compute the missing values using the principal components\n",
        "\n",
        "* New equations is as follows\n",
        "  1. Create a matrix if the value is missing fill it with the mean of that column. If we have it then keep it.\n",
        "  1. Repeat the following until our objective increases.\n",
        "    1. Calculate the principal components off the matrix in 1 like we did in 12.2\n",
        "    1. Now for each value that we took the mean for, we set it equal to (1)\n",
        "    1. Perform (2) with all the values that we did not estimate for in the matrix\n",
        "  1. Return the missing entries.\n",
        "\n",
        "* We usually have to guess M to perform this algorithm. We can test how well these performed by using a validation set and using only some of the data for the algo.\n",
        "    "
      ],
      "metadata": {
        "id": "roFhbs5H7Knm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principal Components with Missing Values Cont.\n",
        "(1) $∑_{m=1}^M a_{im}b_{jm}$\n",
        "\n",
        "(2) $∑(x_{ij} - ∑_{m=1}^M a_{im}b_{jm})^2$"
      ],
      "metadata": {
        "id": "FrKANsOFBbf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recommender Systems\n",
        "* Companies like Amazon and Netflix use this data to recommend users movies or products based on reviews from what they have watched and bought. Comparing this to what others used have rated from their bought and watched. It then uses a similar algorithm and fills in missing data using PC. Then it can recommend movies or products that got a high missing data number(say each thing was ranked 1-5.)"
      ],
      "metadata": {
        "id": "SSI5zzGCB1Q8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.4 Clustering Methods\n",
        "\n",
        "* We want to cluster data that is similar to each other into a cluster. Then we want different clusters for data that differs from one another.\n",
        "\n",
        "* Clustering goal is to find homogeneous subgroups while PCA goal is find a low dimensional represenative of the observation that explains a good portion of the variance.\n",
        "\n",
        "* 2 most common clustering methods are *k mean clustering* and *hierarchical clustering*\n",
        "\n",
        "* K means clustering predetermined number of clusters then breaks the data into those\n",
        "\n",
        "* Hierarchical Clustering uses a tree like visual and doesn't predetermine a set amount of groups. Uses a dendrogram that allows a visual of all the clustering."
      ],
      "metadata": {
        "id": "PBFSX0LWFMoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.4.1 K Mean Clustering\n",
        "\n",
        "* Break the data down into K distinct nonoverlapping clusters.\n",
        "\n",
        "* Thus there are two properties of the $K$ clusters on $n$ observations.\n",
        "  1. The union of $C_1 ..... C_K = 1 ...... n $\n",
        "  1. The intersection of $C_1 .....C_K = ∅$\n",
        "* How to judge the strength of a cluster? The goal is to minimize the variation inside a cluster. Minimize ($∑_{k=1}^K W(C_K)$) Where $W(C_K)$ is the within cluster variation.\n",
        "\n",
        "* We normally used Squared Euclidean Distance for $W(C_K)$\n",
        "\n",
        "* The algorithm works as follows\n",
        "  1. Pick a $K$ value than randomly assign each point to a cluster in range ${1...K}$.\n",
        "  1. Iterate through this step until the cluster assingment doesn't change.\n",
        "    1. For each of the $K$ clusters compute the cluster centroid. The $k_{th}$ cluster centroid is the vector of the $p$ feature means for the observations in the $k_{th}$ cluster.\n",
        "    1. Assign each observation to the centroid that is the closest using Euclidean Distance\n",
        "* This will minimize the equation above when plugging $W(C_K)$ in. However you must try several different values of $K$ and see which performs the best. Also you must run each Algo several times since the first step is random to find the smallest global maxima for that K value, not the local maxima."
      ],
      "metadata": {
        "id": "lqxXz88IFQM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.4.2 Hierarchical Clustering\n",
        "\n",
        "* Advantages are the dendrogram is easier to read and visualize. And we don't have to preselect a $K$ value at the start.\n",
        "\n",
        "* Normally the dendrogram is built from the leaves up to the drunk. Hence bottom-up approach is called agglomerative."
      ],
      "metadata": {
        "id": "lfPJoTcy09fj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interpreting a Dendrogram\n",
        "\n",
        "* Each line in the dendrogram represents a leaf and when two leaves merge it creates a branch. Higher up the graph several branches can merge. The higher up the graph a merge occurs the less similar the two groups are. The lower the merge occurs the more similar the two groups are.\n",
        "\n",
        "* Be careful when reading the Dendrogram to make sure that when two branches meet everything that is combined has equal probability. Not the nearest number on the x axis. That would be incorrect\n",
        "\n",
        "* This hierarchical structure works well for data that follows a hierarchical structure. However sometimes it will perform worse than K Means clustering when the data isn't hierarchical. Like gender, nationality, etc."
      ],
      "metadata": {
        "id": "fz8WzQYh2qbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hierarchical Clustering Algorithm\n",
        "\n",
        "* First we must define a dissimilarity function which we will use Euclidean Distance. There are other options\n",
        "\n",
        "* Each $n$ observation is defined as its own cluster. We then compare each group to each other using the dissimilarity function. The smallest number achieved will be the two groups merged. We continue this until all the groups are merged.\n",
        "\n",
        "* However how do we determine the dissimilarity of a group with multiple members vs a single member group? This is called linkage and there is several types.\n",
        "  1. Complete: Compute all dissimilarities between members of group A and B and take the maximium.\n",
        "  1. Single: Take all the dissimilarities between group A and group B and take the minimum. Can result in a ton of single linkage.\n",
        "  1. Average: All the interactions between the dissimilarities between group A and group B and take the average.\n",
        "  1. Centroid: Take the dissimilarity of centroid A from centroid B(centroid is a mean vector p). Can result in inversions.\n"
      ],
      "metadata": {
        "id": "dho95opX5A-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Choice of dissimilarity measure\n",
        "\n",
        "* Alternative to Euclidean Distance is Correlated-based distance.\n",
        "\n",
        "* Correlated-based distance uses features to tell if two observations are similar, even though their values could be far apart in terms of Euclidean Distance.\n",
        "\n",
        "* Correlated-based distance relies less on magnitude and more on shape.\n",
        "\n",
        "* Example be a recommended product search for shoppers. Algorithmn uses 0 if you never bought the item and 1 if you have. Better to use Correlated-based distance since you would want to group shoppers on similar items, not group shoppers on how much they shop.\n",
        "\n",
        "* Also must consider if you should scale the mean to 0 and the standard deviation to 1. This way the units won't play a role or the practicality of an item for the shopping example. Like buying 1 computer a year or 20 pairs of socks a year."
      ],
      "metadata": {
        "id": "RvD9tp6A7cGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.4.3 Practical Issues in Clustering\n",
        "\n",
        "1. Small Decisions with big consequences\n",
        "  1. Must decide to standardize the values or not\n",
        "  1. Using K mean clustering you must predetermine $K$.\n",
        "  1. Using Hierarchical Clustering you must pick the dissimilarity used, linkage used, and where to cut the dendrogram to cluster.\n",
        "1. Validating the Cluster Obtained.\n",
        "  1. Can sometimes use a validation set and try the algorithm on that. See if it performs as well.\n",
        "1. Other Considerations in Clustering\n",
        "  1. Mixture Models- are a solution to the K mean clustering and Hierarchical Clustering having to categorize every observation. Sometimes there is a high leverage or outlier that shouldn't be categorized. Thus the mixture model is a soft version of both and doesn't need to categorize every observation. **Also fails when we retest and it removes different observations**\n",
        "1. Tempered Approach to Interpreting the Results of Clustering\n",
        "  1. Clustering is not very Robust. Hence one should try several different parameters for $K$, linkage, dissimilarity, scaling parameters, cutting the dendrogram etc. Then try to find robustness in smaller steps and use those to start to form a hypothesis about the data.\n",
        "  "
      ],
      "metadata": {
        "id": "1YiFted69m6o"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZhBYOGNCAMZo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}