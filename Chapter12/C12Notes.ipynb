{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 12 Unsupervised Learning\n",
        "\n",
        "* Book has primarily involved supervised learning like regression and classification. Where we have a response $Y$ for a set of $n$ observations and $p$ predictors.\n",
        "\n",
        "* Now we will not have a response $Y$ and instead look to find meaningful observations about the data.\n",
        "\n",
        "* Main 2 types we will focus on are\n",
        "  * principal component analysis - tool used for data visualization\n",
        "  * clustering - method for discovering subgroups in the data"
      ],
      "metadata": {
        "id": "Apmm2_c61B0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.1 The Challenge of Unsupervised Learning\n",
        "\n",
        "* We can't check our work like we did with supervised learning. We don't have an outcome variable.\n",
        "\n",
        "* Unsupervised learning can be useful in such cases\n",
        "\n",
        "  * We know a shoppers previous online shopping history and want to show similar items that would group in with those\n",
        "  * We examine the genes of breast cancer patients and try to find a grouping of potentially bad genes\n"
      ],
      "metadata": {
        "id": "lJR7fhOT1KXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.2 Principal Component Analysis\n",
        "\n",
        "* Similar to chapter 6 Principal Component Regression. When we do this regression we must select the principal components.\n",
        "\n",
        "* PCA - (Principal Component Analysis) -  is the process of selecting these principal components\n",
        "\n",
        "* Remember PCR goal is to summarize the space with the fewest variables for the largest space."
      ],
      "metadata": {
        "id": "DgliB-T3HeZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.2.1 What are principal components\n",
        "\n",
        "* One way to visualize correlation is to use scatter plots. However, if $p$ is large then $p\\choose 2$ will be unbearable. If $p$ is small then this is doable.\n",
        "\n",
        "* We would want to capture as much as the data as possible in a smaller dimension.\n",
        "\n",
        "* PCA does this as it tries to capture a smaller amount of the interesting predictors. It finds predictors more interesting the more they vary across each dimension.\n",
        "  * Each of the dimensions found by PCA is a linear combination of the all $p$ features\n",
        "  * The first principal component of a set of features $X_1 ,....., X_p$ is the normalized combination of the features $Z_1 = ϕ_{11}X_1 + ϕ_{21}X_2+....+ ϕ_{p1}X_p$ and normalized means $ ∑_{j=1}^{p} Φ_{j1}^2 = 1$. This limits the size of the values so the variance isn't extremely large.\n",
        "  * We refer to $Φ_{11}$ as the loadings and $ϕ_{1} = (ϕ_{11}+ ϕ_{21}+....+ ϕ_{p1})^T$\n",
        "  * Imagine an  n x p  matrix. Look for linear combinations of sample feature of the form  $z_{i1} = ϕ_{11}x_{11} + ϕ_{21}x_{21}+....+ ϕ_{p1}x_{p1}$ subject to the constraint listed above.\n",
        "  * Thus we must maximize the following equation for $ϕ_{11}+ ϕ_{21}+....+ ϕ_{p1}$ ,Max = $\\frac {1}{n} ∑_{i=1}^n(∑_{j=1}^p ϕ_{j1}x_{ij})^2$ subject to the constraint above\n",
        "  * The Second Principal Component is the linear combination of $X_1 ...... X_p$ that has the max variance out of all the linear combinations that are uncorrelated to $Z_1$ gives us $Z_2$\n",
        "  * $z_{i2} = ϕ_{12}x_{i1} + ϕ_{22}x_{i2}+....+ ϕ_{p2}x_{ip}$\n",
        "  * To be uncorrelated $Z_1$ must be orthogonal to $Z_2$.\n",
        "  * Once we have computed all principal components we can plot them against each other. This greatly reduces the amount of variables to plot.\n",
        "  * Can also determine the values for $PC1$ and $PC2$. If the numbers are large and similar then that predictor is important and correlated. If it is less than 0 then it is below average, and 0 is standard."
      ],
      "metadata": {
        "id": "_JtAF6IeJJpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.2.2 Another Interpretation of Principal Components\n",
        "* Our goal is to make a hyperplane with n principal components in n dimensions. We will minimize Euclidean distance. The smallest sum of squares will be our answer\n",
        "* This would give us a formula to find the smallest value of the following equation where $M = $ number of principal components.\n",
        "  * $ ∑_{i=1}^n∑_{j=1}^p (x_{ij} - ∑_{m=1}^M z_{im}ϕ_{jm})^2 $\n",
        "* This is a great method when M is larger and be perform better than the variance formula above"
      ],
      "metadata": {
        "id": "rUk2J9vSJqZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.2.3 The proportion of variance explained\n",
        "* We want to understand how much information is lost in projecting the observations onto the first few principal components. Aka how much variance is not contained in the first few principal components.\n",
        "  * More generally we are interested in the proportion of variance explained (POE)\n",
        "* The PVE can be interpreted as the $R^2$ value .\n",
        "* Per each Principal Component it will explain less variance of the data but cumulating all $M$ principal components should eventually explain 100 % of the variance.\n"
      ],
      "metadata": {
        "id": "wULIg7pidanI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.2.4 More on PCA"
      ],
      "metadata": {
        "id": "sjUHoxighG1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scaling the Variables\n",
        "* The variables need to have a mean of 0\n",
        "* Variables should also be multiplied by a constant since their units can all be different. This would lead to several different variances and we want the variances to be relatively the same. This way the results per principal component are judged fairly.\n",
        "* If all variables were measured in the same units then we might not want to scale everything to standard deviation 1.\n",
        "* We scale everything to standard deviation 1 as it is difficult to arbitrarily scale units of different magnitudes."
      ],
      "metadata": {
        "id": "u69PtdZJ11r1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Uniqueness of Principal Components\n",
        "* Principal Components are almost always unique and the sign bit doesn't matter. IT can be negative or positive. The result is the same."
      ],
      "metadata": {
        "id": "D7FmCmRM3Glk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deciding how many Principal Components to use\n",
        "* One way to is to plot the amount of variance each Principal component explains and stop pick a point where it drops off.\n",
        "\n",
        "* Another is to explore the data til we stop seeing uniqueness from the principal components and stop it there."
      ],
      "metadata": {
        "id": "ajw1an563sha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.2.5 Other Uses for Principal Components\n",
        "* Many other techniques like regression , classification, and clustering you can use n x M instead of n x p. Where M is the principal component score vector. This reduces the noise in the data and makes computations less intensive."
      ],
      "metadata": {
        "id": "0ltaPq8h5Fk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.3 Missing Values and Matrix Completion\n",
        "* First we could remove the entire row if an element $x_{ij}$ is missing\n",
        "\n",
        "* This is awful wasteful so instead we could replace that point by the mean of the $j$ column. This works well but there is better ways.\n",
        "\n",
        "* We can use principal components in a method called matrix completion to fill in the missing data.\n",
        "\n",
        "* This works well with recommender systems to recommend users new movies or products."
      ],
      "metadata": {
        "id": "2yweI4U05yFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principal Components with Missing Values\n",
        "* We go back to the previous section of computing Principal components and use that equation. However, it becomes much more difficult to solve since we can't use eigenvalues and vectors since there is missing values.\n",
        "\n",
        "* Once we solve that equation we can compute the missing values using the principal components\n",
        "\n",
        "* New equations is as follows\n",
        "  1. Create a matrix if the value is missing fill it with the mean of that column. If we have it then keep it.\n",
        "  1. Repeat the following until our objective increases.\n",
        "    1. Calculate the principal components off the matrix in 1 like we did in 12.2\n",
        "    1. Now for each value that we took the mean for, we set it equal to (1)\n",
        "    1. Perform (2) with all the values that we did not estimate for in the matrix\n",
        "  1. Return the missing entries.\n",
        "\n",
        "* We usually have to guess M to perform this algorithm. We can test how well these performed by using a validation set and using only some of the data for the algo.\n",
        "    "
      ],
      "metadata": {
        "id": "roFhbs5H7Knm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Principal Components with Missing Values Cont.\n",
        "(1) $∑_{m=1}^M a_{im}b_{jm}$\n",
        "\n",
        "(2) $∑(x_{ij} - ∑_{m=1}^M a_{im}b_{jm})^2$"
      ],
      "metadata": {
        "id": "FrKANsOFBbf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recommender Systems\n",
        "* Companies like Amazon and Netflix use this data to recommend users movies or products based on reviews from what they have watched and bought. Comparing this to what others used have rated from their bought and watched. It then uses a similar algorithm and fills in missing data using PC. Then it can recommend movies or products that got a high missing data number(say each thing was ranked 1-5.)"
      ],
      "metadata": {
        "id": "SSI5zzGCB1Q8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.4 Clustering Methods"
      ],
      "metadata": {
        "id": "PBFSX0LWFMoT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lqxXz88IFQM7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}