{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 12 Unsupervised Learning\n",
        "\n",
        "* Book has primarily involved supervised learning like regression and classification. Where we have a response $Y$ for a set of $n$ observations and $p$ predictors.\n",
        "\n",
        "* Now we will not have a response $Y$ and instead look to find meaningful observations about the data.\n",
        "\n",
        "* Main 2 types we will focus on are\n",
        "  * principal component analysis - tool used for data visualization\n",
        "  * clustering - method for discovering subgroups in the data"
      ],
      "metadata": {
        "id": "Apmm2_c61B0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.1 The Challenge of Unsupervised Learning\n",
        "\n",
        "* We can't check our work like we did with supervised learning. We don't have an outcome variable.\n",
        "\n",
        "* Unsupervised learning can be useful in such cases\n",
        "\n",
        "  * We know a shoppers previous online shopping history and want to show similar items that would group in with those\n",
        "  * We examine the genes of breast cancer patients and try to find a grouping of potentially bad genes\n"
      ],
      "metadata": {
        "id": "lJR7fhOT1KXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.2 Principal Component Analysis\n",
        "\n",
        "* Similar to chapter 6 Principal Component Regression. When we do this regression we must select the principal components.\n",
        "\n",
        "* PCA - (Principal Component Analysis) -  is the process of selecting these principal components\n",
        "\n",
        "* Remember PCR goal is to summarize the space with the fewest variables for the largest space."
      ],
      "metadata": {
        "id": "DgliB-T3HeZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.2.1 What are principal components\n",
        "\n",
        "* One way to visualize correlation is to use scatter plots. However, if $p$ is large then $p\\choose 2$ will be unbearable. If $p$ is small then this is doable.\n",
        "\n",
        "* We would want to capture as much as the data as possible in a smaller dimension.\n",
        "\n",
        "* PCA does this as it tries to capture a smaller amount of the interesting predictors. It finds predictors more interesting the more they vary across each dimension.\n",
        "  * Each of the dimensions found by PCA is a linear combination of the all $p$ features\n",
        "  * The first principal component of a set of features $X_1 ,....., X_p$ is the normalized combination of the features $Z_1 = ϕ_{11}X_1 + ϕ_{21}X_2+....+ ϕ_{p1}X_p$ and normalized means $ ∑_{j=1}^{p} Φ_{j1}^2 = 1$. This limits the size of the values so the variance isn't extremely large.\n",
        "  * We refer to $Φ_{11}$ as the loadings and $ϕ_{1} = (ϕ_{11}+ ϕ_{21}+....+ ϕ_{p1})^T$\n",
        "  * Imagine an  n x p  matrix. Look for linear combinations of sample feature of the form  $z_{i1} = ϕ_{11}x_{11} + ϕ_{21}x_{21}+....+ ϕ_{p1}x_{p1}$ subject to the constraint listed above.\n",
        "  * Thus we must maximize the following equation for $ϕ_{11}+ ϕ_{21}+....+ ϕ_{p1}$ ,Max = $\\frac {1}{n} ∑_{i=1}^n(∑_{j=1}^p ϕ_{j1}x_{ij})^2$ subject to the constraint above\n",
        "  * The Second Principal Component is the linear combination of $X_1 ...... X_p$ that has the max variance out of all the linear combinations that are uncorrelated to $Z_1$ gives us $Z_2$\n",
        "  * $z_{i2} = ϕ_{12}x_{i1} + ϕ_{22}x_{i2}+....+ ϕ_{p2}x_{ip}$\n",
        "  * To be uncorrelated $Z_1$ must be orthogonal to $Z_2$.\n",
        "  * Once we have computed all principal components we can plot them against each other. This greatly reduces the amount of variables to plot.\n",
        "  * Can also determine the values for $PC1$ and $PC2$. If the numbers are large and similar then that predictor is important and correlated. If it is less than 0 then it is below average, and 0 is standard."
      ],
      "metadata": {
        "id": "_JtAF6IeJJpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.2.2 Another Interpretation of Principal Components\n",
        "* Our goal is to make a hyperplane with n principal components in n dimensions. We will minimize Euclidean distance. The smallest sum of squares will be our answer\n",
        "* This would give us a formula to find the smallest value of the following equation where $M = $ number of principal components.\n",
        "  * $ ∑_{i=1}^n∑_{j=1}^p (x_{ij} - ∑_{m=1}^M z_{im}ϕ_{jm})^2 $\n",
        "* This is a great method when M is larger and be perform better than the variance formula above"
      ],
      "metadata": {
        "id": "rUk2J9vSJqZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.2.3 The proportion of variance explained"
      ],
      "metadata": {
        "id": "wULIg7pidanI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sjUHoxighG1A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}