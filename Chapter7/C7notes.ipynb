{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 7 - Moving Beyond Linearity"
      ],
      "metadata": {
        "id": "jRkKPjaf6HxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.0 Introduction to the Model Methods\n",
        "\n",
        "* We are going to explore other methods that extend past linearity like polynomial regression, step functions, regression splines, smoothing splines, local regression, generative additive model.\n",
        "\n",
        "1. **Polynomial Regression** - take the linear model and add extra predictors raised to a certain power\n",
        "\n",
        "1. **Step Functions** - cut the  $X$ into $K$ pieces inorder to produce a qualitive function. Makes it into a piece wise function\n",
        "\n",
        "1. **Regression Splines** -  combination of the previous two. $X$ is cut into $K$ pieces then on these pieces we apply polgnomial regression. The polynomials are constrained so they combined smoothly at these piecewise regions called *knots*.\n",
        "\n",
        "1. **Smoothing Splines** - smiliar to regression splines, but result from minimizing the residual sum of squares subject to the smoothnes penalty.\n",
        "\n",
        "1. **Local Regression** - similar to the splines except the regions are now allowed to overlap.\n",
        "\n",
        "1. **Generative Additive Model** - allows us to extend the methods above to deal with muliple predictors"
      ],
      "metadata": {
        "id": "WbhnbHGz6SqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1 Polynomial Regression\n",
        "\n",
        "* Take the linear formula of $ y = β_{0} + β_{1}X_{i} + ϵ $\n",
        "\n",
        "\n",
        "* Transform it into Polynomial Version $ y = β_{0} + β_{1}X_{i} + β_{2}X_{i}^{2} ...... β_{n}X_{i}^{n} + ϵ $\n"
      ],
      "metadata": {
        "id": "ndlBd_LU65_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2 Step Functions\n",
        "\n",
        "* Polynomial Regression involves impossing a gloabal structure on the non linear function $X$ instead we want to break $X$ into bins and fit a different constant for each bin. So we break a continuous variable into an ordered categorical variable.\n",
        "\n",
        "* Assume we make k cuts with $c_{1}..... c_{k}$ and construct $K+1$ variables:\n",
        "\n",
        ">> $ C_{0} = I(X <c_{1})$\n",
        "\n",
        ">> $ C_{1} = I(c_{1}≤ X < c_{2}) $\n",
        "\n",
        ">> $ . $\n",
        "\n",
        ">> $ . $\n",
        "\n",
        ">> $ C_{k} = I(c_{k} ≤ X) $\n",
        "\n",
        "* $I$ is a indicator variable that returns 1 if the statement is true and 0 if it is false. Thus the above function of $ C_{0} + C_{1} + ... + C_{K} = 1 $ will always be true.\n",
        "\n",
        "* Apply our linear formula from 7.1.0 to the piecewise above to get\n",
        "\n",
        ">> $ y = β_{0} + β_{1}C_{1}(x_{i}) + ..... + β_{k}C_{k}(x_{i}) $\n",
        "\n",
        "* This formula can also be applied to logistic regression.\n"
      ],
      "metadata": {
        "id": "-B03BNZG99nt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3 Basis Functions\n",
        "\n",
        "* Polynomial and Step functions belong to the basis functions class\n",
        "\n",
        "* Basis function is a function that takes the form of\n",
        "\n",
        ">> $ y = β_{0} + β_{1}b_{1}(x_{i}) + ..... + β_{k}b_{k}(x_{i}) $\n",
        "\n",
        "* Many other choices for basis functions like wavelets or Fourier series. And the next section regression splines.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wOTIhtL2Aleh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4 Regression Splines\n",
        "\n",
        "* Class of basis functions that combined the polynomial and step functions we just learned about."
      ],
      "metadata": {
        "id": "A9ro98cPCeWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.1 Piecewise Polynomials\n",
        "\n",
        "* Let pick a polynomial of X = 3 then we fit our standard polynomail expression to be\n",
        "  * $ y = β_{0} + β_{1}X_{i} + β_{2}X_{i}^{2} + β_{3}X_{i}^{3} + ϵ $\n",
        "  * Each point where we change the coeficients are called knots and rely on our piecewise breakdown\n",
        "  * Lets assume we break down $K = 2$ and a split $c$ so $y =$\n",
        "     \\begin{cases}\n",
        "       β_{01} + β_{11}X_{i} + β_{21}X_{i}^{2} + β_{31}X_{i}^{3} + ϵ & if x_{i} < c  \\\\\n",
        "      β_{02} + β_{12}X_{i} + β_{22}X_{i}^{2} + β_{32}X_{i}^{3} + ϵ & if x_{i} ≥ c\n",
        "     \\end{cases}\n",
        "* The higher the K the more knots thus more flexible the model.\n",
        "\n",
        "* However this method can lead to non continuos piece wise functions.\n"
      ],
      "metadata": {
        "id": "WHhDW9nzDaMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.2 Constraints and Splines\n",
        "\n",
        "* **Constraint 1** The fitted curve must be continuous. We can make this change but it can sometimes result in a weird curve like a $V$.\n",
        "\n",
        "* **Constraint 2** We fix the above by making sure the first and second derivatives of the piecewise functions are continuous.\n",
        "\n",
        "* These constraints are freeing up degrees of freedom since we are making certain parts of our above equation equal to 0 by taking derivates. Thus with Constraint 1 and Constaint 2 we have freed up 3 degrees of freedom.\n",
        "\n",
        "* Previously we had 8 degrees of freedom since each equation had 4 degrees of freedom and we have one c value so 2 splits. Now we have 5 degrees of freedom.\n",
        "\n",
        "* This creates a *cubic spline*. Normally a cubic spline with $K$ knots use $4+ K$ degrees of freedom.\n",
        "\n",
        "* *Linear Spline* is a piecewise $d$ degree polynomial with continunity in derivates up to $d-1$ at each knot."
      ],
      "metadata": {
        "id": "FNQqqUeeEIJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.3 The Spline Basis Representation\n",
        "\n",
        "* Use the basis formula to represent a *cubic spline* with $K$ knots\n",
        "   * $ y = β_{0} + β_{1}b_{1}(x_{i}) + ..... + β_{K+3}b_{K+3}(x_{i}) $\n",
        "* Now normally use the above formula than add one truncated power formula per knot.\n",
        "  * Turns it to zero if $x ≤ ξ$ else it performs $(x - ξ)^{3}$\n",
        "  * Thus it adds an extra term which we call $h(x, ξ)$\n",
        "  * We need to add an exta term per knot thus we get K +4 knots for a degree 3 polynomial\n",
        "  * Also this wont effect constraint 1,2 since it is to the power 3. It would only not be continuous at derivate 3.\n",
        "  \n",
        "* *Natural Spline* is a regression spline with addinional boundary constraint. The function is required to be linear at the boundaries. (Boundary knots are the two where $X< c_{0}$ and $ X≥ c_{k}$)\n",
        "\n",
        "* Natural splines are better at estimating values at end of the boundaries\n",
        "\n",
        "* Splines are great but can take on high variance at the outter end of the predictors so just use natural splines for those values"
      ],
      "metadata": {
        "id": "fc3Z9ONLIcxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.4 Choosing the Number and Location of the Knots\n",
        "\n",
        "* Could place more knots in higher varying data or just place them uniformly across the function.\n",
        "\n",
        "* Normally the knots are just spread out evenly over the data.\n",
        "\n",
        "* How many knots to pick? One option is to pick several values and look at the graphs. Pick the smoothest, most continuous curve. This is rather subjective tho\n",
        "\n",
        "* More objective way is to use cross validation. Train-Test-Split the data then on the training set us Cross Validation with a certain k prob 5,10. Try a bunch of different values of knots and see which gives the small RSS.\n",
        "\n",
        "* With muliple predictors (variables) we typically don't do this CV as it would require a large computational power. Instead we just pick a value"
      ],
      "metadata": {
        "id": "yeUz6y6dJXSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.5 Comparison to Polynomial Regression\n",
        "\n",
        "* Cubic Splines normally outperform Polynomial Regression. This is because the Polynomial regression might have to take on a high degree polynomial like 17. However with cubic splines we would still only got up to 3. We would compensate with K knots however. This allows us to fit a better approach since we can adjust each of those terms.\n",
        "\n",
        "* Also with cubic splines we can add more knots at areas of a high degree of change and less knots at constant areas. Polynomial regression can not do this."
      ],
      "metadata": {
        "id": "fpwN4CdVOJKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.5 Smoothing Splines\n",
        "\n",
        "* Regression Splines we picked a certain number of knots, producing a sequence of K basis functions, then used least square to estimate the spline coefficients. Now we will go over a different way to get the formula for a spline."
      ],
      "metadata": {
        "id": "1KUCjwXtO5f8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.5.1 An Overview of Smoothing Splines\n",
        "\n",
        "* We want to find a function $g(x)$ that fits the data well to minimize the RSS. However if we don't constrain $g(x)$ then we will get an exact fit which will overfit the data.\n",
        "\n",
        "* So we want to make $g(x)$ smooth and make RSS small. How do we do this? One formual is\n",
        " * $∑_{i=1}^{n}(y_{i} - g(x))^2 - λ∫g^{''}(t)^2dt$\n",
        " *  λ is a non negative turning parameter\n",
        "  * The formula takes on the form of loss + penalty.\n",
        "    * The loss is the RSS equation\n",
        "    * The penalty is the new λ term\n",
        "    * The second derivate states the change of the slope. The more wiggly then the higher the value. The less wiggly the smaller the value.\n",
        "    * The intergeral is the summuation over the range of t.\n",
        "    * Thus the second term can be thought of as the summation of the change of the slope. So if it is jumpy then it is a high penalty, if it is smooth then a low penalty.\n",
        "    * λ = 0 then it will be extremely jumpy\n",
        "    * λ = ∞ then it will be a straight line\n",
        "    * Thus need to pick a λ that accurately represents the function and data\n",
        "  * The function that equates to all of this is a natural cubic spline with knots at each unique value of x with shrunked values. So close to the formula in 7.4 with more knots but shrunked in regards to the λ given"
      ],
      "metadata": {
        "id": "JmtsBhOXPxzb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.5.2 Choosing the Smoothing Parameter λ\n",
        "\n",
        "* It might seem that smoothing spline will have too many knots since we have one at each x. This will lead to extremely high degree of freedom.\n",
        "\n",
        "* However the λ controls the number of effective degree of freedom. We use this over degree of freedom since we are shrinking some of these terms.\n",
        "\n",
        "* Thus we need to pick an effective λ to create the most optimal effective degrees of freedom.\n",
        "\n",
        "* We can use Cross Validation and futher Leave one out cross validation which performs extremely fast in this situation."
      ],
      "metadata": {
        "id": "dagbm_9OQ7Q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.6 Local Regression\n",
        "\n",
        "* Compute the fit of a target point$x_{0}$ by using other neighboring points within a threshold.\n",
        "\n",
        "* We need to pick a span to perform the algorithm. We can calculate the optimal span with cross validation. Span is the porpotion of points used to calculate the fit at a certain point.\n",
        "  * Also need to calculate a weight, but a constant can be given\n",
        "  * The higher the s the more wiggly the fit\n",
        "  * Smaller the s the more rigid.\n",
        "\n",
        "* Local regression is very useful in a global setting where all the variables share a common globabl variable and are local in a another. Like time\n",
        "\n",
        "* Works well in a 2 dimensional model when 2 variables are related and parametic in nature. However like K nearest neighbors it suffers in high dimensionality and shouldn't be used over p = 3 or p =4 .\n",
        "\n",
        "* Different that k nearest neights since we assign less weight value to points futher away and pick a span s which is a portion of the values choosen. Then perform least squares regression for each x."
      ],
      "metadata": {
        "id": "cb-PsU-0TEHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.7 Generative Additive Models\n",
        "\n",
        "* previous sections we learned a bunch of models to build of the basis functions of linearity with 1 variable. However if we have X predictors? Then we will use Generative Additive Models (GAM)\n",
        "\n",
        "* GAM can be applied with quanative and qualitive response.\n",
        "\n",
        "* GAM takes extends the linear model by allowing non linear functions of each variable while maintaining additivity"
      ],
      "metadata": {
        "id": "oCWW7197YN_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.7.1 GAMS for Regression Problems Quantitative\n",
        "\n",
        "* A natural way to extend the linear formula is\n",
        "  * $ y = β_{0} + β_{1}X_{i1} ....... + β_{n}X_{in}+ ϵ $\n",
        "  * Replace each equation $β_{j}X_{ij}$ with a smooth non linear function $f_{j}X_{ij}$\n",
        "* Now we can write it as\n",
        "  * $ y = β_{0} + ∑_{j=1}^{n}f_{j}(X_{ij}) + e_{i}$\n",
        "  * We call this model additive since we are adding each predictors function together.\n",
        "* Thus since we have an additive model we can select several different methods to discover $f_{j}X_{ij}$.\n",
        "  * We can use smoothing splines on one, regression splines on another, logistic regression, hot encoding, dummy variables etc.\n",
        "\n",
        "* Advantage from additive is we can keep all the other function variables fixed and experiment with one predictor at a time.\n",
        "\n",
        "* Disadvantage is since it is additive it can miss important interaction between variables, which we can add it. Can also add low dimensional interaction between variables manually."
      ],
      "metadata": {
        "id": "CKfHDIzWZdFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.7.2 GAMS for classification Problems"
      ],
      "metadata": {
        "id": "RgczNP_9aD9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The pros and cons are the exact same as 7.7.1\n",
        "\n",
        "* The formula is the same since we take the log odds of the normal logistic regression formula.\n",
        "\n",
        "  * $ log( \\frac {p(X)}{1- p(X)}) = β_{0} + ∑_{j=1}^{n}f_{j}(X_{ij}) $\n",
        "\n",
        "* We can fit each $f_{j}X_{ij}$ individually with the same constraints as the one above"
      ],
      "metadata": {
        "id": "xymsEz7uceRf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FC9rMYmGdVu2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}